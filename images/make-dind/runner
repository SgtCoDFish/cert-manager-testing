#!/usr/bin/env bash

# Copyright 2023 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# generic runner script, handles DIND, etc.


# Check if the job has opted-in to local caching and if so check for a cache and
# copy the cache to prepopulate the local cache. After the job is done, change the
# latest cache directory to the local cache directory.
export LOCAL_CACHE_ENABLED=${LOCAL_CACHE_ENABLED:-false}

if [[ "${LOCAL_CACHE_ENABLED}" == "true" ]]; then
    if [[ "${SHARED_CACHE_DIR}" == "" ]]; then
        echo >&2 "LOCAL_CACHE_ENABLED was enabled but SHARED_CACHE_DIR is empty."
        exit 1
    fi

    if [[ "${LOCAL_CACHE_DIR}" == "" ]]; then
        echo >&2 "LOCAL_CACHE_ENABLED was enabled but LOCAL_CACHE_DIR is empty."
        exit 1
    fi

    # Convert local cache directory to absolute path and re-export it.
    LOCAL_CACHE_DIR="$(pwd)/${LOCAL_CACHE_DIR}"
    export LOCAL_CACHE_DIR

    if [[ -f "${SHARED_CACHE_DIR}/latest" ]]; then
        echo "Local cache [restore]: found latest cache directory."

        # Obtain the name of the latest cache directory.
        latest_cache_dir=$(cat "${SHARED_CACHE_DIR}/latest")

        mkdir -p "${LOCAL_CACHE_DIR}"

        # Copying the latest cache to our local cache ...
        rsync -avv --inplace --delete "${latest_cache_dir}/." "${LOCAL_CACHE_DIR}"

        echo "Local cache [restore]: provisioned ${LOCAL_CACHE_DIR}"
    else
        echo "Local cache [restore]: no latest cache directory found."
    fi
fi

# Check if the job has opted-in to docker in docker and if so start the docker daemon
export DOCKER_IN_DOCKER_ENABLED=${DOCKER_IN_DOCKER_ENABLED:-false}

if [[ "${DOCKER_CONFIG:-}" != "" ]]; then
    if [[ "${DOCKER_IN_DOCKER_ENABLED}" != "true" ]]; then
        echo >&2 "DOCKER_CONFIG was requested but DOCKER_IN_DOCKER_ENABLED is not true."
        exit 1
    fi

    echo "A writable DOCKER_CONFIG was requested."
    tmpdir="$(mktemp -d)"
    ln -s "${DOCKER_CONFIG}/config.json" "${tmpdir}/config.json"
    export DOCKER_CONFIG="${tmpdir}"
fi

if [[ "${EXTRA_DOCKER_OPTS:-}" != "" ]]; then
    if [[ "${DOCKER_IN_DOCKER_ENABLED}" != "true" ]]; then
        echo >&2 "EXTRA_DOCKER_OPTS was requested but DOCKER_IN_DOCKER_ENABLED is not true."
        exit 1
    fi

    echo "DOCKER_OPTS=\"\${DOCKER_OPTS} ${EXTRA_DOCKER_OPTS}\"" >>/etc/default/docker
fi

if [[ "${DOCKER_IN_DOCKER_ENABLED}" == "true" ]]; then
    echo >&2 "Initializing Docker in Docker."

    service docker start
    # The service may be marked as ready but the Docker socket may not be
    # ready yet.
    WAIT_N=0
    MAX_WAIT=5
    while true; do
        # docker ps -q should only work if the daemon is ready
        docker ps -q >/dev/null 2>&1 && break
        if [[ ${WAIT_N} -lt ${MAX_WAIT} ]]; then
            WAIT_N=$((WAIT_N + 1))
            echo >&2 "Waiting for docker to be ready, sleeping for ${WAIT_N} seconds."
            sleep ${WAIT_N}
        else
            echo >&2 "Reached maximum attempts, not waiting any longer..."
            break
        fi
    done
fi

# Disable error exit so we can run post-command cleanup.
set +o errexit

# Run the actual job.
"$@" &

# Bash does not "trikle down" UNIX signals. If the Bash script receives SIGINT
# coming from Prow due to the 2 hours timeout being hit, and that the above
# command "$@" is running, then SIGINT won't be passed down to the "$@" command.
# To work around that, we trap SIGINT and SIGTERM and pass then down
# explicitely. The reasons for handling both SIGTERM and SIGINT is detailed in
# the following table:
#
#  |                          Reason                          |   Signal    |
#  |----------------------------------------------------------|-------------|
#  | The 2 hours Prow timeout has been reached                | SIGINT [1]  |
#  | Google Cloud VM preempted using ACPI shutdown            | SIGTERM [2] |
#  | GKE worker removed due to scale down using ACPI shutdown | SIGTERM [2] |
#
#  [1]: https://github.com/kubernetes/test-infra/blob/ee1e7c8/prow/entrypoint/run.go#L242
#  [2]: https://unix.stackexchange.com/questions/499761/what-signal-is-sent-to-running-programs-scripts-on-shutdown
#
# shellcheck disable=SC2064
trap "kill -s INT $!" INT
# shellcheck disable=SC2064
trap "kill -s TERM $!" TERM
wait $!

EXIT_VALUE=$?

# cleanup after job
if [[ "${DOCKER_IN_DOCKER_ENABLED}" == "true" ]]; then
    echo "Stopping docker ..."
    service docker stop || true
fi

if [[ "${LOCAL_CACHE_ENABLED}" == "true" ]]; then
    if [[ $EXIT_VALUE == 0 ]]; then
        cache_unique_id="cache_$(date +"%F_H%H-M%M-S%S")_$(head -c 8 /proc/sys/kernel/random/uuid)"

        # 0. Make sure the local cache dir and the unique shared dir exist.
        mkdir -p "${LOCAL_CACHE_DIR}"
        mkdir -p "${SHARED_CACHE_DIR}/${cache_unique_id}"

        # 1. Copy the latest shared cache directory to the new shared directory that we are creating.
        #    This should be a same-disk rsync and should be fast. This forms the basis of the new cache.
        if [[ -f "${SHARED_CACHE_DIR}/latest" ]]; then
            latest_cache_dir=$(cat "${SHARED_CACHE_DIR}/latest")

            echo "Local cache [update]: Copying latest cache to new cache directory ..."
            rsync -avv --inplace "${latest_cache_dir}/." "${SHARED_CACHE_DIR}/${cache_unique_id}"
        fi

        # 2. Copy the local cache directory to the new shared directory that we are creating. rsync
        #    will only copy the files that are not already present in the shared directory. The new
        #    shared directory now contains the latest cache + what was downloaded in the current job.
        echo "Local cache [update]: Copying local cache to shared cache ..."
        rsync -avv --inplace "${LOCAL_CACHE_DIR}/." "${SHARED_CACHE_DIR}/${cache_unique_id}"

        # 3. Update the latest cache directory to the local cache directory.
        echo "Local cache [update]: Updating latest cache directory to ${SHARED_CACHE_DIR}/${cache_unique_id}"
        echo "${SHARED_CACHE_DIR}/${cache_unique_id}" > "${SHARED_CACHE_DIR}/latest"

        # 4. Remove the old cache directories to save disk space. Keep the
        #    last 4 cache directories because they may be used by other
        #    jobs that are still copying from these directories.
        echo "Local cache [update]: Removing old caches ..."
        find "${SHARED_CACHE_DIR}" -maxdepth 1 -type d -name 'cache_*' -printf '%f\n' | \
            sort -r | \
            tail -n +4 | \
            xargs -I{} rm -rf "${SHARED_CACHE_DIR}/{}"
    else
        echo "Local cache [update]: Job failed, not updating cache."
    fi
fi

# preserve exit value from job / bootstrap
exit ${EXIT_VALUE}
